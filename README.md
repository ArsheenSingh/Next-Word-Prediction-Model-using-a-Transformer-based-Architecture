# Next Word Prediction Model using a Transformer based Architecture🚀 
<!DOCTYPE html>
<html lang="en">
<body>  
  <h2>📄 Description</h2>
  <p>
    This project implements a next word prediction model using a transformer-based architecture built with TensorFlow. Trained on the <a href="./data/Oxford_English_Text.txt">Oxford English Text</a> dataset, the model generates contextually relevant word predictions for a given input. It showcases advanced NLP techniques for sequence modeling and text generation.
  </p>
  
  <h2>✨ Interesting Techniques</h2>
  <ul>
    <li>🔍 <strong>Transformer Architecture:</strong> Utilizes self-attention and positional encoding for contextual learning.</li>
    <li>📝 <strong>Text Preprocessing:</strong> Cleans and tokenizes raw text data using regular expressions. Learn more about <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions" target="_blank">Regular Expressions</a> on MDN.</li>
    <li>⚙️ <strong>Sequence Modeling:</strong> Trains the model to predict subsequent words based on learned language patterns.</li>
  </ul>
  
  <h2>🔧 Technologies & Libraries</h2>
  <ul>
    <li>🐍 <strong>Python:</strong> Core programming language for model development.</li>
    <li>🤖 <strong>TensorFlow & Keras:</strong> Frameworks used for building and training the transformer model.</li>
    <li>📚 <strong>NumPy:</strong> For efficient numerical computations.</li>
    <li>🔤 <strong>NLTK (or similar):</strong> For text tokenization and preprocessing.</li>
  </ul>
  
  <h2>📁 Project Structure</h2>
  <p>
    <strong>notebooks:</strong> Main development environment with the Jupyter Notebook :  <a href="Next_word_predictor_Final.ipynb". <br>
    <strong>data:</strong> Contains the text dataset used for training :  <a href="Oxford English Text.txt" . <br>
    <strong>models:</strong> Houses saved checkpoints and model weights after training :  <a href="tokenizer1.pkl" and next_words.karas (file too large to be uploaded on GitHub). <br>
  </p>
</body>
</html>
